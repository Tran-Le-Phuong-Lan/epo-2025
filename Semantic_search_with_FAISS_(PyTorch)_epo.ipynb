{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfPCA-ShVh3w"
      },
      "source": [
        "# Semantic search with FAISS (PyTorch)\n",
        "\n",
        "**Reference:** \n",
        "\n",
        "[1] This notebook provided by Hugging Face: https://huggingface.co/learn/llm-course/en/chapter5/6\n",
        "\n",
        "[2] FAISS: https://github.com/facebookresearch/faiss/wiki/Getting-started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DqrLM8iVh3y"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install datasets evaluate transformers[sentencepiece]\n",
        "\n",
        "# reference:\n",
        "# [1] https://stackoverflow.com/questions/58957169/faiss-error-could-not-find-a-version-that-satisfies-the-requirement-faiss-from/58957380\n",
        "# [1] Self-summary: \n",
        "#   1.1 Python version too high (for example: 3.13 has problem with installing faiss)\n",
        "#   1.2 must state the cuda version explicitly while installing faiss -> (after install torch) check 'nvidia-...' version in  `conda list > requirement.txt` \n",
        "# ! pip install faiss-gpu-cu12 # "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7MZuJ0ZxVh3z",
        "outputId": "924fb96f-9c2a-438b-eafd-3fec14a46cfc"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
        "# issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "row counts in df_1: 1848\n",
            "row counts in df_2: 22\n",
            "row counts in df_epo: 1870\n"
          ]
        }
      ],
      "source": [
        "df_1 = pd.read_csv(\"/home/lephuonglantran/EPO2024/df_combine.csv\")\n",
        "print(f\"row counts in df_1: {len(df_1)}\")\n",
        "df_2 = pd.read_csv(\"/home/lephuonglantran/EPO2024/df_combine_val.csv\")\n",
        "print(f\"row counts in df_2: {len(df_2)}\")\n",
        "df = pd.concat([df_1, df_2], ignore_index=True)\n",
        "\n",
        "print(f\"row counts in df_epo: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lephuonglantran/.conda/envs/gpuenv_2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# convert panda data frame to dataset\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'claims', 'ipc'],\n",
              "    num_rows: 1870\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = Dataset.from_pandas(df)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kwS04TSIVh30",
        "outputId": "df1da230-3518-42c1-8936-d4d8b8d502bf"
      },
      "outputs": [],
      "source": [
        "# issues_dataset = issues_dataset.filter(\n",
        "#     lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
        "# )\n",
        "# issues_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We can see that there are a lot of columns in our dataset, most of which we don’t need to build our search engine. From a search perspective, the most informative columns are `title`, `body`, and `comments`, while `html_url` provides us with a link back to the source issue. Let’s use the `Dataset.remove_columns()` function to drop the rest:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KQPnvx7pVh30",
        "outputId": "277f3ac1-4c34-46ca-ae94-c0032c045dcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc'],\n",
              "    num_rows: 1870\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# columns = issues_dataset.column_names\n",
        "# columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
        "# columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
        "# issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
        "# issues_dataset\n",
        "\n",
        "columns = dataset.column_names\n",
        "columns_to_keep = [\"title\", \"description\", \"ipc\"]\n",
        "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
        "dataset = dataset.remove_columns(columns_to_remove)\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Now that we **have one comment per row**, let’s **create a new comments_length column** that **contains the number of words per comment**:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "u1V7Gs2lVh31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1870/1870 [00:02<00:00, 714.13 examples/s] \n"
          ]
        }
      ],
      "source": [
        "# comments_dataset = comments_dataset.map(\n",
        "#     lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
        "# )\n",
        "\n",
        "description_dataset = dataset.map(\n",
        "    lambda x: {\"description_length\": len(x[\"description\"].split())}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We can **use this new column to filter out short comments**, which typically **include things like “cc @lewtun” or “Thanks!” that are not relevant** for our search engine. There’s **no precise number to select for the filter**, **but around 15 words** seems like a good start:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vrAOWkChVh31",
        "outputId": "d45e7de1-f384-4a1c-c997-0534f917b12e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 1870/1870 [00:00<00:00, 16374.73 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc', 'description_length'],\n",
              "    num_rows: 1870\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
        "# comments_dataset\n",
        "\n",
        "description_dataset = description_dataset.filter(lambda x: x[\"description_length\"] > 15)\n",
        "description_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kgHWo25UVh31"
      },
      "outputs": [],
      "source": [
        "# def concatenate_text(examples):\n",
        "#     return {\n",
        "#         \"text\": examples[\"title\"]\n",
        "#         + \" \\n \"\n",
        "#         + examples[\"description\"]\n",
        "#     }\n",
        "\n",
        "# # comments_dataset = comments_dataset.map(concatenate_text)\n",
        "# description_dataset = description_dataset.map(concatenate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47423"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "description_dataset[0]['description_length']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "min length in dataset: 359 words\n",
            "max length in dataset: 21706\n"
          ]
        }
      ],
      "source": [
        "max = 0\n",
        "for i in range(len(description_dataset)):\n",
        "    length_description_dataset = description_dataset[i]['description_length']\n",
        "    if i== 0:\n",
        "        min = length_description_dataset\n",
        "    if length_description_dataset <= min:\n",
        "        min = length_description_dataset\n",
        "    else:\n",
        "        max = length_description_dataset\n",
        "print(f\"min length in dataset: {min} words\\nmax length in dataset: {max}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split the long description into small chunks\n",
        "\n",
        "reference:\n",
        "\n",
        "[1] https://saturncloud.io/blog/how-to-split-text-in-a-column-into-multiple-rows-using-pandas/\n",
        "\n",
        "[2] joing list of words into a string: https://stackoverflow.com/questions/67560768/join-list-element-after-split-into-str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def spilt_into_smaller_descriptions(examples):\n",
        "    res = []\n",
        "    index = 0\n",
        "    num_words_per_chunk = 359\n",
        "    total_chunks = examples[\"description\"].split()\n",
        "    total_len = examples[\"description_length\"]\n",
        "    while index < total_len:\n",
        "        chunk = ' '.join(total_chunks[index: index+num_words_per_chunk]) \n",
        "                        # the elem with index = index + num_words_per_chunk \n",
        "                        # is excluded\n",
        "        res.append(chunk)\n",
        "        index = index + num_words_per_chunk\n",
        "    last_chunk = ' '.join(total_chunks[index - num_words_per_chunk: total_len])\n",
        "    res.append(last_chunk)\n",
        "    return {\n",
        "        \"description\": res\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reference: https://discuss.huggingface.co/t/how-can-i-grab-the-first-n-rows-of-a-dataset-as-a-dataset-object/33093/2\n",
        "# small_sample = description_dataset.select(range(10))\n",
        "# small_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# small_sample[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# small_sample = small_sample.map(spilt_into_smaller_descriptions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# small_sample[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1870/1870 [00:03<00:00, 494.21 examples/s]\n"
          ]
        }
      ],
      "source": [
        "description_dataset = description_dataset.map(spilt_into_smaller_descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "convert to dataframe to use `explode`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# small_sample.set_format(\"pandas\")\n",
        "# df_small_sample = small_sample[:]\n",
        "description_dataset.set_format(\"pandas\")\n",
        "df_description_dataset = description_dataset[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>ipc</th>\n",
              "      <th>description_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS</td>\n",
              "      <td>FIELD OF INVENTIONThe present invention relate...</td>\n",
              "      <td>C</td>\n",
              "      <td>47423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS</td>\n",
              "      <td>kits based on the present invention may be sui...</td>\n",
              "      <td>C</td>\n",
              "      <td>47423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS</td>\n",
              "      <td>example, the presence, absence or quantity of ...</td>\n",
              "      <td>C</td>\n",
              "      <td>47423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS</td>\n",
              "      <td>the stuffer sequence may have about 1 to about...</td>\n",
              "      <td>C</td>\n",
              "      <td>47423</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        title  \\\n",
              "0  METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS   \n",
              "1  METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS   \n",
              "2  METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS   \n",
              "3  METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS   \n",
              "\n",
              "                                         description ipc  description_length  \n",
              "0  FIELD OF INVENTIONThe present invention relate...   C               47423  \n",
              "1  kits based on the present invention may be sui...   C               47423  \n",
              "2  example, the presence, absence or quantity of ...   C               47423  \n",
              "3  the stuffer sequence may have about 1 to about...   C               47423  "
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df_small_sample_explode = df_small_sample.explode(\"description\", ignore_index=True)\n",
        "# df_small_sample_explode.head(4)\n",
        "df_description_dataset_explode = df_description_dataset.explode(\"description\", ignore_index=True)\n",
        "df_description_dataset_explode.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert the dataframe back to dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc', 'description_length'],\n",
              "    num_rows: 60154\n",
              "})"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "description_dataset = Dataset.from_pandas(df_description_dataset_explode)\n",
        "description_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating text embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "yKcqAREOVh32"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModel.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "nX3lDtF-Vh32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MPNetModel(\n",
              "  (embeddings): MPNetEmbeddings(\n",
              "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): MPNetEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x MPNetLayer(\n",
              "        (attention): MPNetAttention(\n",
              "          (attn): MPNetSelfAttention(\n",
              "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (intermediate): MPNetIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): MPNetOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (relative_attention_bias): Embedding(32, 12)\n",
              "  )\n",
              "  (pooler): MPNetPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > As we mentioned earlier, we’d **like to represent each entry in our GitHub issues corpus as a single vector**, so we **need to “pool” or average our token embeddings** in some way. One popular approach is to **perform CLS pooling on our model’s outputs**, where we **simply collect the last hidden state for the special [CLS] token**. The following function does the trick for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5fDzb5SWVh32"
      },
      "outputs": [],
      "source": [
        "def cls_pooling(model_output):\n",
        "    return model_output.last_hidden_state[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Next, we’ll create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7vnrkBahVh32"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(text_list):\n",
        "    encoded_input = tokenizer(\n",
        "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "    model_output = model(**encoded_input)\n",
        "    return cls_pooling(model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "27el6788Vh32",
        "outputId": "7ba1f604-edf1-434f-9ab5-5919fc13c3a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
        "# embedding.shape\n",
        "embedding = get_embeddings(description_dataset[\"description\"][0])\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "dO9LHydvVh32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 60154/60154 [24:40<00:00, 40.64 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# embeddings_dataset = comments_dataset.map(\n",
        "#     lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
        "# )\n",
        "embeddings_dataset = description_dataset.map(\n",
        "    lambda x: {\"embeddings\": get_embeddings(x[\"description\"]).detach().cpu().numpy()[0]}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "7RbZJfVZVh32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/61 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 61/61 [00:00<00:00, 147.07it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc', 'description_length', 'embeddings'],\n",
              "    num_rows: 60154\n",
              "})"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0TYuteMRVh32",
        "outputId": "06a98363-a102-4efa-9f1d-7c0666471847"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"How can I load a dataset offline?\"\n",
        "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
        "question_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "42Mxc3zUVh32"
      },
      "outputs": [],
      "source": [
        "scores, samples = embeddings_dataset.get_nearest_examples(\n",
        "    \"embeddings\", question_embedding, k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CEY-C86lVh32"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "samples_df = pd.DataFrame.from_dict(samples)\n",
        "samples_df[\"scores\"] = scores\n",
        "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YNm1oWXmVh32",
        "outputId": "249ba260-bbd1-4e3d-f99e-148f29e721aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
            "\n",
            "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
            "SCORE: 25.505014419555664\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
            "You can now use them offline\n",
            "```python\n",
            "datasets = load_dataset('text', data_files=data_files)\n",
            "```\n",
            "\n",
            "We'll do a new release soon\n",
            "SCORE: 24.5555419921875\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
            "\n",
            "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
            "\n",
            "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
            "\n",
            "----------\n",
            "\n",
            "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
            "\n",
            "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
            "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
            "```python\n",
            "load_dataset(\"./my_dataset\")\n",
            "```\n",
            "and the dataset script will generate your dataset once and for all.\n",
            "\n",
            "----------\n",
            "\n",
            "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
            "cf #1724 \n",
            "SCORE: 24.14897918701172\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
            "> \n",
            "> 1. (online machine)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> import datasets\n",
            "> \n",
            "> data = datasets.load_dataset(...)\n",
            "> \n",
            "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> 2. copy the dir from online to the offline machine\n",
            "> \n",
            "> 3. (offline machine)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> import datasets\n",
            "> \n",
            "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> \n",
            "> \n",
            "> HTH.\n",
            "\n",
            "\n",
            "SCORE: 22.893997192382812\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
            "1. (online machine)\n",
            "```\n",
            "import datasets\n",
            "data = datasets.load_dataset(...)\n",
            "data.save_to_disk(/YOUR/DATASET/DIR)\n",
            "```\n",
            "2. copy the dir from online to the offline machine\n",
            "3. (offline machine)\n",
            "```\n",
            "import datasets\n",
            "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
            "```\n",
            "\n",
            "HTH.\n",
            "SCORE: 22.406654357910156\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for _, row in samples_df.iterrows():\n",
        "    print(f\"COMMENT: {row.comments}\")\n",
        "    print(f\"SCORE: {row.scores}\")\n",
        "    print(f\"TITLE: {row.title}\")\n",
        "    print(f\"URL: {row.html_url}\")\n",
        "    print(\"=\" * 50)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save and reload FAISS database\n",
        "\n",
        "**references:**\n",
        "\n",
        "[1] https://huggingface.co/docs/datasets/v1.2.0/faiss_and_ea.html\n",
        "\n",
        "[2] https://discuss.huggingface.co/t/save-and-load-datasets/9260"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving the dataset (1/1 shards): 100%|██████████| 60154/60154 [00:00<00:00, 392558.11 examples/s]\n"
          ]
        }
      ],
      "source": [
        "description_dataset.save_to_disk('epo_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\n",
        "embeddings_dataset.save_faiss_index('embeddings', 'epo_index.faiss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds = load_dataset('crime_and_punish', split='train[:100]')\n",
        "# ds.load_faiss_index('embeddings', 'my_index.faiss')\n",
        "from datasets import load_from_disk\n",
        "load_dataset = load_from_disk('./epo_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dataset.load_faiss_index('embeddings', 'epo_index.faiss')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Semantic search with FAISS (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpuenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
