{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfPCA-ShVh3w"
      },
      "source": [
        "# Semantic search with FAISS (PyTorch)\n",
        "\n",
        "**Reference:** \n",
        "\n",
        "[1] This notebook provided by Hugging Face: https://huggingface.co/learn/llm-course/en/chapter5/6\n",
        "\n",
        "[2] FAISS: https://github.com/facebookresearch/faiss/wiki/Getting-started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DqrLM8iVh3y"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install datasets evaluate transformers[sentencepiece]\n",
        "\n",
        "# reference:\n",
        "# [1] https://stackoverflow.com/questions/58957169/faiss-error-could-not-find-a-version-that-satisfies-the-requirement-faiss-from/58957380\n",
        "# [1] Self-summary: \n",
        "#   1.1 Python version too high (for example: 3.13 has problem with installing faiss)\n",
        "#   1.2 must state the cuda version explicitly while installing faiss -> (after install torch) check 'nvidia-...' version in  `conda list > requirement.txt` \n",
        "# ! pip install faiss-gpu-cu12 # "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install peft \n",
        "# ! pip install joblib\n",
        "# ! pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lephuonglantran/.conda/envs/gpuenv_2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7MZuJ0ZxVh3z",
        "outputId": "924fb96f-9c2a-438b-eafd-3fec14a46cfc"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
        "# issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "row counts in df_1: 1848\n",
            "row counts in df_2: 22\n",
            "row counts in df_epo: 1870\n"
          ]
        }
      ],
      "source": [
        "df_1 = pd.read_csv(\"/home/lephuonglantran/EPO2024/df_combine.csv\")\n",
        "print(f\"row counts in df_1: {len(df_1)}\")\n",
        "df_2 = pd.read_csv(\"/home/lephuonglantran/EPO2024/df_combine_val.csv\")\n",
        "print(f\"row counts in df_2: {len(df_2)}\")\n",
        "df = pd.concat([df_1, df_2], ignore_index=True)\n",
        "\n",
        "print(f\"row counts in df_epo: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df[\"claims\"][0], df[\"title\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert panda data frame to dataset\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'claims', 'ipc'],\n",
              "    num_rows: 1870\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = Dataset.from_pandas(df)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kwS04TSIVh30",
        "outputId": "df1da230-3518-42c1-8936-d4d8b8d502bf"
      },
      "outputs": [],
      "source": [
        "# issues_dataset = issues_dataset.filter(\n",
        "#     lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
        "# )\n",
        "# issues_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We can see that there are a lot of columns in our dataset, most of which we don’t need to build our search engine. From a search perspective, the most informative columns are `title`, `body`, and `comments`, while `html_url` provides us with a link back to the source issue. Let’s use the `Dataset.remove_columns()` function to drop the rest:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KQPnvx7pVh30",
        "outputId": "277f3ac1-4c34-46ca-ae94-c0032c045dcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc'],\n",
              "    num_rows: 1870\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# columns = issues_dataset.column_names\n",
        "# columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
        "# columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
        "# issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
        "# issues_dataset\n",
        "\n",
        "columns = dataset.column_names\n",
        "columns_to_keep = [\"title\", \"description\", \"ipc\"]\n",
        "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
        "dataset = dataset.remove_columns(columns_to_remove)\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Now that we **have one comment per row**, let’s **create a new comments_length column** that **contains the number of words per comment**:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u1V7Gs2lVh31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 1870/1870 [00:02<00:00, 681.33 examples/s] \n"
          ]
        }
      ],
      "source": [
        "# comments_dataset = comments_dataset.map(\n",
        "#     lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
        "# )\n",
        "\n",
        "description_dataset = dataset.map(\n",
        "    lambda x: {\"description_length\": len(x[\"description\"].split())}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We can **use this new column to filter out short comments**, which typically **include things like “cc @lewtun” or “Thanks!” that are not relevant** for our search engine. There’s **no precise number to select for the filter**, **but around 15 words** seems like a good start:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vrAOWkChVh31",
        "outputId": "d45e7de1-f384-4a1c-c997-0534f917b12e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 1870/1870 [00:00<00:00, 16375.00 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc', 'description_length'],\n",
              "    num_rows: 1870\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
        "# comments_dataset\n",
        "\n",
        "description_dataset = description_dataset.filter(lambda x: x[\"description_length\"] > 15)\n",
        "description_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kgHWo25UVh31"
      },
      "outputs": [],
      "source": [
        "# def concatenate_text(examples):\n",
        "#     return {\n",
        "#         \"text\": examples[\"title\"]\n",
        "#         + \" \\n \"\n",
        "#         + examples[\"description\"]\n",
        "#     }\n",
        "\n",
        "# # comments_dataset = comments_dataset.map(concatenate_text)\n",
        "# description_dataset = description_dataset.map(concatenate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47423"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "description_dataset[0]['description_length']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "min length in dataset: 359 words\n",
            "max length in dataset: 195429\n"
          ]
        }
      ],
      "source": [
        "max = 0\n",
        "for i in range(len(description_dataset)):\n",
        "    length_description_dataset = description_dataset[i]['description_length']\n",
        "    if i== 0:\n",
        "        min = length_description_dataset\n",
        "        max = length_description_dataset\n",
        "    else:\n",
        "        if length_description_dataset <= min:\n",
        "            min = length_description_dataset\n",
        "        \n",
        "        if length_description_dataset >= max:\n",
        "            max = length_description_dataset\n",
        "print(f\"min length in dataset: {min} words\\nmax length in dataset: {max}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split the long description into small chunks\n",
        "\n",
        "reference:\n",
        "\n",
        "[1] https://saturncloud.io/blog/how-to-split-text-in-a-column-into-multiple-rows-using-pandas/\n",
        "\n",
        "[2] joing list of words into a string: https://stackoverflow.com/questions/67560768/join-list-element-after-split-into-str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def spilt_into_smaller_descriptions(examples):\n",
        "    res = []\n",
        "    index = 0\n",
        "    num_words_per_chunk = 359\n",
        "    total_chunks = examples[\"description\"].split()\n",
        "    total_len = examples[\"description_length\"]\n",
        "    while index < total_len:\n",
        "        chunk = ' '.join(total_chunks[index: index+num_words_per_chunk]) \n",
        "                        # the elem with index = index + num_words_per_chunk \n",
        "                        # is excluded\n",
        "        res.append(chunk)\n",
        "        index = index + num_words_per_chunk\n",
        "    last_chunk = ' '.join(total_chunks[index - num_words_per_chunk: total_len])\n",
        "    res.append(last_chunk)\n",
        "    return {\n",
        "        \"description\": res\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc', 'description_length'],\n",
              "    num_rows: 20\n",
              "})"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reference: https://discuss.huggingface.co/t/how-can-i-grab-the-first-n-rows-of-a-dataset-as-a-dataset-object/33093/2\n",
        "# small_sample = description_dataset.select(range(20))\n",
        "small_sample = description_dataset.select(range(20, 40))\n",
        "small_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# small_sample[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 20/20 [00:00<00:00, 461.86 examples/s]\n"
          ]
        }
      ],
      "source": [
        "sm_description_dataset = small_sample.map(spilt_into_smaller_descriptions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sm_description_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# description_dataset = description_dataset.map(spilt_into_smaller_descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "convert to dataframe to use `explode`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# small_sample.set_format(\"pandas\")\n",
        "# df_small_sample = small_sample[:]\n",
        "\n",
        "# description_dataset.set_format(\"pandas\")\n",
        "# df_description_dataset = description_dataset[:]\n",
        "\n",
        "sm_description_dataset\n",
        "sm_description_dataset.set_format(\"pandas\")\n",
        "df_sm_description_dataset = sm_description_dataset[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>ipc</th>\n",
              "      <th>description_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>INHALATION PARTICLES: method of preparation</td>\n",
              "      <td>The present invention relates to methods for t...</td>\n",
              "      <td>A</td>\n",
              "      <td>5279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>INHALATION PARTICLES: method of preparation</td>\n",
              "      <td>It is well acknowledged that the adhesion and ...</td>\n",
              "      <td>A</td>\n",
              "      <td>5279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>INHALATION PARTICLES: method of preparation</td>\n",
              "      <td>conditions. However, solid state properties (p...</td>\n",
              "      <td>A</td>\n",
              "      <td>5279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>INHALATION PARTICLES: method of preparation</td>\n",
              "      <td>when all conventional techniques have failed. ...</td>\n",
              "      <td>A</td>\n",
              "      <td>5279</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         title  \\\n",
              "0  INHALATION PARTICLES: method of preparation   \n",
              "1  INHALATION PARTICLES: method of preparation   \n",
              "2  INHALATION PARTICLES: method of preparation   \n",
              "3  INHALATION PARTICLES: method of preparation   \n",
              "\n",
              "                                         description ipc  description_length  \n",
              "0  The present invention relates to methods for t...   A                5279  \n",
              "1  It is well acknowledged that the adhesion and ...   A                5279  \n",
              "2  conditions. However, solid state properties (p...   A                5279  \n",
              "3  when all conventional techniques have failed. ...   A                5279  "
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df_small_sample_explode = df_small_sample.explode(\"description\", ignore_index=True)\n",
        "# df_small_sample_explode.head(4)\n",
        "\n",
        "# df_description_dataset_explode = df_description_dataset.explode(\"description\", ignore_index=True)\n",
        "# df_description_dataset_explode.head(4)\n",
        "\n",
        "df_sm_description_dataset_explode = df_sm_description_dataset.explode(\"description\", ignore_index=True)\n",
        "df_sm_description_dataset_explode.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert the dataframe back to dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc', 'description_length'],\n",
              "    num_rows: 826\n",
              "})"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# description_dataset = Dataset.from_pandas(df_description_dataset_explode)\n",
        "# description_dataset\n",
        "\n",
        "sm_description_dataset = Dataset.from_pandas(df_sm_description_dataset_explode)\n",
        "sm_description_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating text embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yKcqAREOVh32"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "# model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "model_ckpt = \"sadickam/sdg-classification-bert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModel.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nX3lDtF-Vh32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > As we mentioned earlier, we’d **like to represent each entry in our GitHub issues corpus as a single vector**, so we **need to “pool” or average our token embeddings** in some way. One popular approach is to **perform CLS pooling on our model’s outputs**, where we **simply collect the last hidden state for the special [CLS] token**. The following function does the trick for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5fDzb5SWVh32"
      },
      "outputs": [],
      "source": [
        "def cls_pooling(model_output):\n",
        "    return model_output.last_hidden_state[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Next, we’ll create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7vnrkBahVh32"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(text_list):\n",
        "    # encoded_input = tokenizer(\n",
        "    #     text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    # )\n",
        "    encoded_input = tokenizer(\n",
        "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "    model_output = model(**encoded_input)\n",
        "    return cls_pooling(model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "27el6788Vh32",
        "outputId": "7ba1f604-edf1-434f-9ab5-5919fc13c3a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
        "# embedding.shape\n",
        "\n",
        "# embedding = get_embeddings(description_dataset[\"description\"][0])\n",
        "# embedding.shape\n",
        "\n",
        "embedding = get_embeddings(sm_description_dataset[\"description\"][0])\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "dO9LHydvVh32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 826/826 [00:10<00:00, 77.78 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# embeddings_dataset = comments_dataset.map(\n",
        "#     lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
        "# )\n",
        "embeddings_dataset = sm_description_dataset.map(\n",
        "    lambda x: {\"embeddings\": get_embeddings(x[\"description\"]).detach().cpu().numpy()[0]}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "7RbZJfVZVh32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 330.16it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc', 'description_length', 'embeddings'],\n",
              "    num_rows: 826\n",
              "})"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "0TYuteMRVh32",
        "outputId": "06a98363-a102-4efa-9f1d-7c0666471847"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# question = \"How can I load a dataset offline?\"\n",
        "question = \"How to test nucleic acids in a sample\"\n",
        "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
        "question_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "42Mxc3zUVh32"
      },
      "outputs": [],
      "source": [
        "scores, samples = embeddings_dataset.get_nearest_examples(\n",
        "    \"embeddings\", question_embedding, k=5\n",
        ")\n",
        "\n",
        "# Loaded version\n",
        "# scores, samples = load_dataset.get_nearest_examples(\n",
        "#     \"embeddings\", question_embedding, k=5\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "CEY-C86lVh32"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "samples_df = pd.DataFrame.from_dict(samples)\n",
        "samples_df[\"scores\"] = scores\n",
        "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# samples_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "YNm1oWXmVh32",
        "outputId": "249ba260-bbd1-4e3d-f99e-148f29e721aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TITLE: PHARMACEUTICAL COMPOSITION\n",
            "SCORE: 215.9669189453125\n",
            "DESCRIPTION: analysis was performed using an Agilent XDB-C18 reverse-phase column (250x4.6 mm, 120 Ǻ, 5 µm), thermostated to 50 °C, with detection at 254 nm. Eluent solvents were as follows: solvent A, 95% Acetonitrile, 5% Water, 4.8 mM phosphoric acid; solvent B, 95% Isopropanol, 5% Water, 4.8 mM phosphoric acid. A gradient from 10% B to 70% B was applied during 30 min with a flow rate of 1 ml/min.4.1.3 Liposome surface potentialTau liposomal construct samples were diluted 100-fold with PBS. Analysis was performed using a Zetasizer Nano (Malvern, USA) at 25 °C. Measurement duration and voltage selection were performed in automatic mode, with a typical applied voltage of 50 mV. Data was transformed using the Smoluchowski equation automatically using DTS 5.0 (Malvern) software to calculate the zeta potential. As the tau liposomal constructs are composed of a mixture of DMPC/DMPG/Cholesterol/MPLA at molar ratio of 9:1:7:0.2; the expected net charge will be negative.4.1.4 Conformational analysis by Circular DichroismTau liposomal constructs were diluted (1:1) with PBS to give a final peptide concentration of 18 µM. Liposomes with identical composition but lacking the tau peptide were used as the blank solution for baseline subtraction. CD spectra were acquired on a Jasco-815 spectropolarimeter with a 0.1 cm path length quarzt cuvette (Hellma, Germany) at 23 °C. Measurements were made over a 195-250 nm wavelength range with a 1.0 nm bandwidth and 0.5 nm resolution. A scan speed of 50 nm/min with response time of 1 sec was employed. Blank spectra (from 8 scans) were averaged and substracted from the average of 8 scans of each sample spectra. The obtained spectrum ([θ]obs, degrees) was smoothed after being converted to mean residue molar ellipticity ([θ], degrees cm2 dmol-1) with the equation [θ] = [θ]obsx(MRW/10lc), where MRW is the mean residue molecular weight (MW/number of residues), l is the optical path length (cm) and c is the concentration (g/cm3).4.1.5 ThT fluorescence assayThT fluorescence measurements were acquired on a microplate reader Infinite M200 (Tecan Group Ltd, Switzerland). As a general procedure, Tau liposomal constructs were diluted to different concentrations with PBS (Table 2). Liposomes of same composition but lacking tau peptide were diluted similarly to be used\n",
            "==================================================\n",
            "\n",
            "TITLE: PHARMACEUTICAL COMPOSITION\n",
            "SCORE: 210.95968627929688\n",
            "DESCRIPTION: (3 x 10 min) and the resin was washed with DMF (x3) and DCM (x5). Finally simultaneous resin cleavage and side-chain deprotections were carried out using a mixture of TFA/TIPS/H2O (95:2.5:2.5) (4 mL) during 3.5 h. Trituration from cold diethyl ether gave the crude product T8 as a white solid (50.2 mg, 10 % yield) with a purity of 55% (from HPLC analysis). MALDI-TOF mass spectrometry confirmed the identity of the major product (m/z expected: 3331.17 [MH+], found: 3335.19).2.5: Synthesis of peptide antigen T9The orthogonally protected amino acid Fmoc-Lys(Mtt)-OH (3 eq) was manually loaded to an amide resin (Rink amide MBHA resin, 1 eq, 0.4 mmol) in the presence of PyBOP/HOBt/DIEA in DMF. The resin was then washed with DMF (3 x 1 min). After removing the N-terminal Fmoc group with 25% piperidine in DMF (1 x 1 min and 2 x 15 min), the second residue of Fmoc-Lys(Mtt)-OH (3 eq), was coupled using the same loading conditions. The following 16 amino acids bearing the Fmoc standard side-chain protecting groups were incorporated applying the previously described coupling protocol. The phosphoaminoacids were introduced as monobenzyl esters at the phosphate group. The coupling time was determined by TNBT test or chloranyl test after a Proline. If necessary, a second coupling was performed with 2 eq of Fmoc-aminoacid in the presence of DIC/HOBt or HATU/DIEA. Each coupling step was followed by a wash step with DMF (3 x 1 min), Fmoc removal step with 25% piperidine in DMF (1 x 1 min and 2 x 15 min) and a second wash step with DMF (7 x 1 min). After the coupling of the Thr(PO(OBzl)OH), 0.5% DBU in DMF was used for the Fmoc-deprotection step. The assembly of the peptide sequence finished with the addition of the last two Fmoc-Lys(Mtt)-OH.Then, the Mtt-groups of the terminal lysine residues were selectively cleaved by treatment of the resin (1 eq, 650 mg, 0.156 mmol) with 10 mL of TIPS/TFA/DCM (1:1:98) during several cycles of 10 min. After washing with DCM (x3) and DMF (x3), Palmitic acid (20 eq, 1.01 g, 3.15 mmol) was coupled to those deprotected amino groups using TBTU (20 eq, 814 mg, 3.15\n",
            "==================================================\n",
            "\n",
            "TITLE: Compositions and methods for treating ophthalmic disorders\n",
            "SCORE: 210.28189086914062\n",
            "DESCRIPTION: <221> misc_feature <222> (58)..(60) <223> Xaa can be any naturally occurring amino acid<400> 44\n",
            "==================================================\n",
            "\n",
            "TITLE: Compositions and methods for treating ophthalmic disorders\n",
            "SCORE: 210.28189086914062\n",
            "DESCRIPTION: <221> misc_feature <222> (58)..(60) <223> Xaa can be any naturally occurring amino acid<400> 44\n",
            "==================================================\n",
            "\n",
            "TITLE: METHOD FOR COLORING MATERIALS WITH NATURAL COLORANTS AND ITS ARTICLES\n",
            "SCORE: 162.7891387939453\n",
            "DESCRIPTION: the impregnated material by the coloration method described by the present invention can be afterwards subjected to a step were pH is adjusted between 2 and 6. This post-treatment enables the deep fixation of the color to the substrate/material, to provide excellent results for fastness (keeping the color after more than 30 washing steps) and resistance (friction, sweat, etc). The materials prepared according to the present invention are innocuous to the health and environment, having also a high degree of fastness, which gives them appropriate characteristics to several applications, ranging from furniture, hats, shoes, garments, upholstery, among others.In yet another preferred embodiment of the coloration method described in the present invention, the concentration of the mentioned enzyme can vary between 0.1-2000 mg/l; preferentially between 0.2-200 mg/l. More preferentially, the enzyme can be of vegetable, animal or microbial source; were the enzymes can be pyranose oxidases, glucose oxidases, glycerol oxidases, lactate oxidases, pyruvate oxidases, uricases, choline oxidases, sarcosine oxidases, bilirubin oxidases, laccases, tyrosinases, peroxidases, catalases, superoxide dismutases, or their mixtures. Inside the laccases group, better results were obtained for the fungal laccase or Aspergillus laccase.The oxidoreductases promote in situ polymerization, and among this class we can find the laccases, which are enzymes that are capable of oxidizing polyphenolic compounds having as final acceptor of the electrons the oxygen. These enzymes can be found in plants, fungus and some bacteria.The laccases appear as enzymes with better results on the coloration of materials/substrates with natural dyes namely leather and textile substrates, where colored materials with high fastness are obtained, by the proper linkage of the natural precursors and process conditions (temperature, time and pH), enabling the achievement of an extensive range of colors with good fastness.In a preferred embodiment of the present method, the precursors used are compounds that are naturally present in nature such as diamines, aminophenols, phenols, polyphenols, or their mixtures. Still more preferentially, natural polyphenolic extracts such as caffeine, theine, vanilla, vegetable extracts such as mimosa, quebracho, pine, chestnut tree, ginger, Caesalpinia echinata or their mixtures. Since the process occurs in moderate temperature and pH conditions, does not use any pigment (synthetic compounds that are normally recalcitrant) using only\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for _, row in samples_df.iterrows():\n",
        "    print(f\"TITLE: {row.title}\")\n",
        "    print(f\"SCORE: {row.scores}\")\n",
        "    print(f\"DESCRIPTION: {row.description}\")\n",
        "    print(\"=\" * 50)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save and reload FAISS database\n",
        "\n",
        "**references:**\n",
        "\n",
        "[1] https://huggingface.co/docs/datasets/v1.2.0/faiss_and_ea.html\n",
        "\n",
        "[2] https://discuss.huggingface.co/t/save-and-load-datasets/9260"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save\n",
        "\n",
        "IMPORTANT:\n",
        "\n",
        "[1] must save the dataset which contains the corresponding computed embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving the dataset (1/1 shards): 100%|██████████| 826/826 [00:00<00:00, 48132.03 examples/s]\n"
          ]
        }
      ],
      "source": [
        "embeddings_dataset.drop_index('embeddings')\n",
        "embeddings_dataset.save_to_disk('./data_embeddings/epo_dataset2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\n",
        "# embeddings_dataset.save_faiss_index('embeddings', './data_embeddings/epo_index1.faiss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # ds = load_dataset('crime_and_punish', split='train[:100]')\n",
        "# # ds.load_faiss_index('embeddings', 'my_index.faiss')\n",
        "# from datasets import load_from_disk\n",
        "# load_dataset = load_from_disk('./data_embeddings/epo_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load_dataset.load_faiss_index('embeddings', './data_embeddings/epo_index.faiss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Searching in multiple databse using FAISS index\n",
        "\n",
        "[1] https://huggingface.co/learn/cookbook/en/semantic_cache_chroma_vector_database\n",
        "\n",
        "[2] https://www.pinecone.io/learn/series/faiss/faiss-tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_from_disk\n",
        "load_all_dataset = []\n",
        "for i in range(2):\n",
        "    dataset_name = './data_embeddings/epo_dataset' + str(i+1)\n",
        "    loaded_dataset = load_from_disk(dataset_name)\n",
        "    # dataset_faiss_name = './data_embeddings/epo_index' + str(i+1) + '.faiss'\n",
        "    # loaded_dataset.load_faiss_index('embeddings', dataset_faiss_name)\n",
        "    load_all_dataset.append(loaded_dataset)\n",
        "print(len(load_all_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['title', 'description', 'ipc', 'description_length', 'embeddings'],\n",
            "    num_rows: 826\n",
            "}) METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS INHALATION PARTICLES: method of preparation\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    load_all_dataset[1],\n",
        "    load_all_dataset[0]['title'][0],\n",
        "    load_all_dataset[1]['title'][0]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "all_dataset = concatenate_datasets(load_all_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc', 'description_length', 'embeddings'],\n",
              "    num_rows: 1669\n",
              "})"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 261.92it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'description', 'ipc', 'description_length', 'embeddings'],\n",
              "    num_rows: 1669\n",
              "})"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_dataset.add_faiss_index(column=\"embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"How to test nucleic acids in a sample\"\n",
        "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
        "question_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TITLE: COMPOSITIONS FOR INCREASING POLYPEPTIDE STABILITY AND ACTIVITY, AND RELATED METHODS\n",
            "SCORE: 157.3109893798828\n",
            "DESCRIPTION: activity at a temperature between about -20 °C to about 35 °C, wherein said polypeptide is encoded by a nucleic acid sequence having a eukaryotic translation initiation sequence. In some embodiments, the polypeptide is a thermostable protein. In some embodiments, the thermostable protein is an enzyme. In some embodiments, the enzyme is a polymerase, a pyrophosphatase, or a deaminase. In some embodiments, the polymerase is a DNA polymerase I, Thermus aquaticus DNA polymerase I (Taq), or Thermococcus gorgonarius DNA polymerase (Tgo). In some embodiments, the polymerase is a Taq polymerase. In some embodiments, the polymerase is not Taq polymerase. In some embodiments, the pyrophosphatase is Thermoplasma acidophilum pyrophosphatase (TAPP). In some embodiments, the deaminase is Pyrococcus horikoshii dCTP deaminase. In some embodiments, the deaminase is a cytidine deaminase or a deoxycytidine deaminase. In some embodiments, the deaminase is a RNA deaminase or a DNA deaminase. In some embodiments, said eukaryotic translation initiation sequence is a Kozak sequence (GCCGCCACCATGGTC). In some embodiments, said composition comprises both a short form and a long form of said polypeptide. In some aspects, said polypeptide linked to a peptide is at least 70% identical to a polypeptide encoded by the nucleic acid sequence of SEQ ID NO: 4, SEQ ID NO: 5, or SEQ ID NO: 12. In some aspects, said polypeptide is linked to a peptide at least 70% identical to SEQ ID NO: 1, 3 or 10.In some cases, when the enzyme (e.g., Taq polymerase, DNA deaminase, RNA deaminase) is linked to the peptide (e.g., a peptide at least 70% identical to SEQ ID NO: 1) it exhibits at least 20%, 50%, 75%, 80%, 85%, 90%, 95%, or 100% of its activity prior to short-term or long-term exposure to temperatures of from about -20 °C to about 35 °C. In some cases, the exposure occurs for at least 1, 2, 3, 4, 5, 6, or 10 hours, at least 1, 2, 3, 4, 5, or 6 days, or at least 1, 2, 3, 4, 5, 6, or 10 weeks, or at least 1, 2, 3, 4, 5, 6, or 10 months.BRIEF DESCRIPTION OF THE DRAWINGSThe novel features of the embodiments provided herein\n",
            "==================================================\n",
            "\n",
            "TITLE: COMPOSITIONS FOR INCREASING POLYPEPTIDE STABILITY AND ACTIVITY, AND RELATED METHODS\n",
            "SCORE: 157.2648468017578\n",
            "DESCRIPTION: an enzymatic activity after exposure to a temperature of about 20 °C to about 30 °C. In some embodiments, the exposure to a temperature is for greater than 1 day. In some embodiments, the enzymatic activity is greater than about 50% of the activity of the composition prior to the exposure to a temperature of at least about -20 °C to about 35 °C. In some aspects, the peptide is encoded by a nucleotide sequence that is at least 70% identical to SEQ ID NO: 3 or SEQ ID NO: 7.In yet a further aspect, this disclosure provides a fusion polypeptide comprising a first peptide that is at least 70% identical to a peptide encoded by SEQ ID NO: 3 and a second peptide that is at least 70% identical to a peptide encoded by SEQ ID NO: 9. In some embodiments, said first and second peptides are linked to a third peptide. In some embodiments, said first and second peptides are linked to each other. In some embodiments, said linkage is covalent. In some embodiments, said first peptide is linked to the N-terminus of a polypeptide and wherein said second peptide is linked to the C-terminus of said polypeptide.In some embodiments, said second peptide is linked to the C-terminus of said first peptide. In some aspects, said fusion polypeptide had at least 70% identity to a peptide encoded by SEQ ID NO: 7.In yet another aspect, this disclosure provides a method of nucleic acid amplification comprising extending a nucleic acid primer with a mixture comprising a polymerase, wherein the polymerase is linked to a peptide that is at least 70% identical to a peptide encoded by SEQ ID NO: 3, SEQ ID NO: 7,to SEQ ID NO: 9, SEQ ID NO: 15, SEQ ID NO: 17 or SEQ ID NO: 19. In some embodiments, the polymerase is linked at its N-terminus to the peptide. In some embodiments, the polymerase is linked at its C-terminus to the peptide. In some embodiments, the polymerase is a Taq polymerase. In some embodiments, the polymerase exhibits an enzymatic activity after exposure to a temperature between -20 °C and 50 °C. In some\n",
            "==================================================\n",
            "\n",
            "TITLE: METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS\n",
            "SCORE: 153.1874237060547\n",
            "DESCRIPTION: to the present invention if a more sensitive detection is needed in measuring very small copy number changes of a nucleic acid of interest in a sample.Various statistical methods may be applied to calculate the copy number of each target site based on experimental results and determine the copy number of the nucleic acid based on the calculated copy number of each target site. A specific example of statistical analysis of small copy number changes is detailed below in Example 4: \"Detection of chromosome 21 copy number changes.\"For another example, real-time PCR may be used to detect the copy number of each of the plurality of the selected target sites as is known in the art. Real-time PCR may determine the copy number of a target site in a monoplex or multiplex manner. Based on the copy numbers for each of the plurality of target sites, the copy number for the nucleic acid may be determined by methods including, but not limited to, taking the average of the copy numbers of all target sites or the median value of the copy numbers for all target sites, or taking the average of the copy numbers of all target sites or the median value of the copy numbers of all target sites after abandoning some egregious values if desirable.IV. Kits For Multiplex Nucleic Acid Analysis and Small Copy Number Change DetectionIn yet another aspect of the present invention, a kit is provided for multiplex nucleic acid analysis and small copy number change detection. In one embodiment, the kit for assaying nucleic acids in a sample include one or more sets of probes corresponding to a target nucleic acid so that the probes in each set, when hybridized to the target nucleic acid may be ligated to form a third probe. In another embodiment, the kit further includes one or more sets of primers for amplifying the third probe.Each set of probes may include a first probe having a first portion at least partially complementary to a first region of the target nucleic acid and a second portion as a first primer binding site, and a second probe having a first portion\n",
            "==================================================\n",
            "\n",
            "TITLE: COMPOSITIONS FOR INCREASING POLYPEPTIDE STABILITY AND ACTIVITY, AND RELATED METHODS\n",
            "SCORE: 148.95538330078125\n",
            "DESCRIPTION: any probe described herein.In some aspects, a buffer can have a MgCl2 concentration of 12.5 mM in a storage buffer and the reaction concentration can be 2.5 mM MgCl2. In other embodiments, the reaction concentration of MgCl2 can be between 1.5 and 2.5 mM. The concentration of a DNA template can be 1 to 50 ng/microliter.In a reaction using an Evagreen dye, the final reaction concentration of a primer (forward or reverse) can be between 80 and 250 nM. In a reaction not using an Evagreen dye and including a probe, the final concentration of a primer can be 200-400 nM and the final concentration of the probe can be 100 to 250 nM.In a reaction using a proofreading enzyme, the MgCl2 concentration can be 1.5mM, the concentration of a primer (forward or reverse) can be 100 to 300 nM, and the concentration of a template DNA can be 5-50 ng/microliter.Where appropriate, an RNase inhibitor (such as Rnasin) that does not inhibit the activity of the RNase employed in the method can also be included. Any aspect of the methods of the invention can occur at the same or varying temperatures. Preferably, the amplification reactions (particularly, primer extension other than the first and second strand cDNA synthesis steps, and strand displacement) are performed isothermally, which avoids the cumbersome thermocycling process. The amplification reaction is carried out at a temperature that permits hybridization of the primers to the template polynucleotide and primer extension products, and that does not substantially inhibit the activity of the enzymes employed. The temperature can be in the range of about 25°C. to about 85°C., about 30°C to about 80°C., or about 37°C to about 75°C.The oligonucleotide components of the amplification reactions provided herein are generally in excess of the number of target nucleic acid sequence to be amplified. They can be provided at about or at least about any of the following: 10, 102, 104, 106, 108, 1010, 1012 times the amount of target nucleic acid.In one aspects, the foregoing components are added simultaneously at the initiation of the amplification process. In another embodiment, components are added in any order prior to or after appropriate\n",
            "==================================================\n",
            "\n",
            "TITLE: METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS\n",
            "SCORE: 140.75946044921875\n",
            "DESCRIPTION: formation of hybridization complexes with both perfect and imperfect matches. Stringent conditions are sequence-dependent and will be different in different circumstances. Longer sequences hybridize specifically at higher temperatures.A guide to the hybridization of nucleic acids is found in Tijssen, Techniques in Biochemistry and Molecular Biology-Hybridization with Nucleic Acid Probes, \"Overview of principles of hybridization and the strategy of nucleic acid assays\" (1993). Generally, stringent conditions are selected to be about 5-10°C lower than the thermal melting point (Tm) for the specific sequence at defined ionic strength and pH. Stringent conditions may be those in which the salt concentration is less than about 1.0 M sodium ion, typically about 0.01 to 1.0 M sodium ion concentration (or other salts) at pH 7.0 to 8.3 and the temperature is at least about 3°C for short probes (e.g. 10 to 50 nucleotides) and at least about 6°C for long probes (e.g. greater than 50 nucleotides). Stringent conditions may also be achieved with the addition of destabilizing agents such as formamide. In another embodiment, less stringent hybridization conditions are used; for example, moderate or low stringency conditions may be used, as are known in the art. See, e.g., Tijssen, supra.Similarly, variations in buffer composition may be used to elucidate the presence or absence of a mismatch at the detection position. Suitable conditions include, but are not limited to, formamide concentration. Thus, for example, \"low\" or \"permissive\" stringency conditions include formamide concentrations of 0 to 10%, while \"high\" or \"stringent\" conditions utilize formamide concentrations of 40%. Low stringency conditions include NaCl concentrations of 1 M, and high stringency conditions include concentrations of 0.3 M. Furthermore, low stringency conditions include MgCl2 concentrations of 10 mM, moderate stringency as 1-10 mM, and high stringency conditions include concentrations of 1 mM.Ligase catalyzes the covalent bonding between two nucleotides adjacent to each other. The ligation reaction is facilitated by a complementary strand holding the two nucleotides comprising the 3' and 5' ends of two polynucleotides with no gaps between the two ends in close proximity. In addition, the ligation reaction requires that there is a phosphate group exposed on the 5' end or hydroxyl group exposed on the\n",
            "==================================================\n",
            "\n",
            "TITLE: COMPOSITIONS FOR INCREASING POLYPEPTIDE STABILITY AND ACTIVITY, AND RELATED METHODS\n",
            "SCORE: 139.99154663085938\n",
            "DESCRIPTION: nucleic acids at approximately a rate of 2n, where n is the number of cycles.A typical conventional PCR thermal cycling protocol comprises 30 cycles of (a) denaturation at a range of 90°C to 95°C for 0.5 to 1 minute, (b) annealing at a temperature ranging from 50°C to 65°C for 1 to 2 minutes, and (c) extension at 68°C to 75°C for at least 1 minute. Other protocols including but not limited to universal protocol as well as fast cycling protocol can be performed the subject probes as well.Another variation of the conventional PCR that can be performed with the compositions provided herein is \"nested PCR\" using nested primers. The method is preferred when the amount of target nucleic acid in a sample is extremely limited for example, where archival, forensic samples are used. In performing nested PCR, the nucleic acid is first amplified with an outer set of primers capable of hybridizing to the sequences flanking a larger segment of the target nucleic acid. This amplification reaction is followed by a second round of amplification cycles using an inner set of primers that hybridizes to target sequences within the large segment.In some embodiments, compositions disclosed herein can be used in a reverse-transcriptasae PCR reaction (RT-PCR), in which a reverse transcriptase first coverts RNA molecules to double stranded cDNA molecules, which are then employed as the template for subsequent amplification in the polymerase chain reaction. In carrying out RT-PCR, the reverse transcriptase is generally added to the reaction sample after the target nucleic acids are heat denatured. The reaction is then maintained at a suitable temperature (e.g., 30 °C-45°C) for a sufficient amount of time (e.g., 5-60 minutes) to generate the cDNA template before the scheduled cycles of amplification take place. Such reaction is particularly useful for detecting the biological entity whose genetic information is stored in RNA molecules.In some embodiments, compositions provided herein can also be used in ligase chain polymerase chain reaction (LCR-PCR). The method involves ligating the target nucleic acids to a set of primer pairs, each having a target-specific portion and a short anchor sequence unrelated to the target sequences. A second set of primers\n",
            "==================================================\n",
            "\n",
            "TITLE: METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS\n",
            "SCORE: 132.88818359375\n",
            "DESCRIPTION: a copy number change for the nucleic acid. As used herein, a plurality of targets sites refers to more than about 5 target sites, preferably more than about 10 target sites, more preferably more than 100 target sites. The number of target sites may increase to about 100-500 if a more sensitive detection is desirable. A person skilled in the art may decide the number of target sites based on the disclosure of the present invention or empirically according to prior testing results.The measurement of copy numbers for each of the plurality of target sites may be accomplished by many techniques, including a multiplex nucleic acid analysis method similar to the CNV detection method as detailed supra, a multiplex nucleic acid analysis employing DNA sequencing techniques, and real-time PCR.For one example, a multiplex nucleic acid analysis method similar to the CNV detection method as detailed supra is used. Similar to the scheme shown in Figure 5, the copy number of 96 target sites may be measured in a single assay simultaneously. The descriptions of the designs of probes, primers and testing procedures are not repeated here except that the statistical analysis of peak intensities is described below.In some embodiments, the fluorescent peak intensity of each target site in a test sample is measured and compared to a standard peak value of the same target site in the testing system to determine the copy number of the target site. A standard value may be a fluorescent intensity value corresponding to the target site in the specific testing system. A testing system means the whole experimental system including the reagents, primers, probes, procedures and devices used for the testing the copy number changes. One test sample is considered to share the same testing system with another test sample if the whole experimental system is the same except the initial DNA sample to be tested. As such a standard value may be obtained for a specific testing system based on prior testing results of DNA samples. If the DNA samples are from normal or wild-type subject, the standard value is a normal or wild-type standard value. If the DNA samples are from\n",
            "==================================================\n",
            "\n",
            "TITLE: METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS\n",
            "SCORE: 132.75848388671875\n",
            "DESCRIPTION: right probe contains a primer binding site Y. The primer binding site Y may be shared by the right probes for all target nucleic acids. In contrast, the left probe for each target nucleic acid contains a unique primer binding sequence (X1, X2, X3 and X4). The primer binding sequences are incorporated in the ligation products. To amplify the ligation product, a pair of primers is designed for each ligation product corresponding to each target site. Because the left probes have four unique primer binding sites, four unique forward primers (F1, F2, F3 and F4) are designed corresponding to the four unique primer binding sites, respectively. In the example, the forward primer F1 is labeled with FAM-blue fluorescent dye, F2 with VIC-green, F3 with NED-yellow, and F4 with PET-red. The reverse primer R binds to the primer binding site Y. As such, the amplification product for target T01 is labeled with FAM-blue fluorescent dye. And the amplification products for targets T02, T03, and T04 are labeled with VIC-green, NED-yellow, and PET-red, respectively. The fluorescent labeled amplification products may then be analyzed by capillary electrophoresis on the basis of the different fluorescent dyes that each product is labeled with. The amplification products differentially labeled with fluorescent dyes may be distinguished even if the amplification products for different target nucleic acids are of the same length.In another embodiment, the multiplexity of nucleic acid analysis is increased by varying the length of primers used for amplifying ligation products. To vary the length of primers, in one example, a stuffer sequence may be inserted into the primers. The stuffer sequence may be incorporated into the amplification product during the PCR reaction; i.e., the primer may be extended to form the amplification product to incorporate the stuffer sequence. As such, the addition of a stuffer sequence may help distinguish the amplification products on the basis of fragment sizes. In one embodiment as illustrated in Figure 2, a stuffer sequence is inserted in primer R2 so that the amplification product from target T02 has a bigger fragment size than the product from target T01. As shown in Figure 2, to detect target nucleic acids T01\n",
            "==================================================\n",
            "\n",
            "TITLE: METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS\n",
            "SCORE: 126.80269622802734\n",
            "DESCRIPTION: for purpose of detecting small copy number changes of a target nucleic acid, two or more sets of probes are used to hybridize to two or more target sites in the same target nucleic acid, with each set of probes hybridizing to a different target site. The small copy number changes may be, for example, a quantitative variation of the target nucleic acid between two samples. In some instances, the quantitative variation of the target nucleic acid is about 0.1% to about 30%.In some embodiments, for each set of probes hybridizing to each target site in the same target nucleic acid, one or more sets of reference probes are used to hybridize to one or more reference target sites, with each set of reference probes hybridizing to a different reference target site. For example, about 1 to about 100 sets of reference probes are used for an individual target site. In some instances, about 6 sets of reference probes are used for an individual target site.In some embodiments, the same set of primers is used to amplify a group of ligation products (also referred to as the third probes). The ligation products may be formed from one or more sets of probes hybridizing to one or more target sites and from one or more sets of reference probes hybridizing to one or more reference target sites. For example, in one group of ligation products, there may be ligation products formed from about 1 to about 100 sets of probes hybridizing to gene target sites of interest and from about 1 to about 100 sets of reference probes hybridizing to reference target sites.In other embodiments, about 50 to about 500 sets of probes hybridizing to about 50 to about 500 target sites on a target nuceic acid are used to detect small copy number variation of the target nucleic acid in a sample. In this case, multiple groups of ligation products formed from a plurality of probes for targets site and probes for reference sites may be obtained. For each group of ligation products, the same primer pair may be used to amply the ligation products in that group.In some\n",
            "==================================================\n",
            "\n",
            "TITLE: METHOD FOR MULTIPLEX NUCLEIC ACID ANALYSIS\n",
            "SCORE: 97.68807983398438\n",
            "DESCRIPTION: the mixture of the probes and the target nucleic acids may be held at about 90 °C to about 99 °C for about 5 second to about 30 seconds for denaturation, and then at about 4°C to about 58 °C for about 1 miute to 48 hours for hybridization and ligation, and the cycles may be repeated to 100 times. In some preferred embodiments, for example, the mixture of the probes and the target nucleic acids may be held at about 95 °C for about 30 seconds for denaturation, and then at 58 °C for about 4 hours for hybridization and ligation, and the cycles may be repeated 4 times.In other embodiments, there is a gap between the hybridized probes on the target nucleic acid. In some instances, the gap may be filled by another probe that is complementary to the gap in the target nucleic acid. This gap-filling probe is designed so that when it is hybridized to the nucleic acid, its two ends are substantially adjacent to the other two probes. In other instances, the gap may be filled by extending one of the probes hybridized to the target nucleic acid. This probe extension may be carried out using a DNA polymerase and dNTPs. A description of DNA polymerase is described supra. For both gap filling methods, a ligation reaction is performed to ligate the substantially adjacent gap-filling probe with the other two probes or ligate the substantially adjacent extended probe with the other probe to obtain ligation products. The ligation reaction may be carried out with suitable parameters including, but not limited to, the type of the ligase, the amount of the ligase, and the duration of the ligation reaction as describe above. Supra.In some embodiments, the ligation products are then analyzed directly to determine the presence, absence, or quantity of target nucleic acids in the sample. In other embodiments, the ligation products are then amplified to obtain amplification products which are analyzed to determine the presence, absence, or quantity of target nucleic acids in the sample.As used herein, \"amplification\" refers to the increase in the number of copies of a particular nucleic acid. Copies of\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "scores, samples = all_dataset.get_nearest_examples(\n",
        "    \"embeddings\", question_embedding, k=10\n",
        ")\n",
        "samples_df = pd.DataFrame.from_dict(samples)\n",
        "samples_df[\"scores\"] = scores\n",
        "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
        "\n",
        "for _, row in samples_df.iterrows():\n",
        "    print(f\"TITLE: {row.title}\")\n",
        "    print(f\"SCORE: {row.scores}\")\n",
        "    print(f\"DESCRIPTION: {row.description}\")\n",
        "    print(\"=\" * 50)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Semantic search with FAISS (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpuenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
