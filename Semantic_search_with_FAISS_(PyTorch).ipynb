{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfPCA-ShVh3w"
      },
      "source": [
        "# Semantic search with FAISS (PyTorch)\n",
        "\n",
        "**Reference:** \n",
        "\n",
        "[1] This notebook provided by Hugging Face: https://huggingface.co/learn/llm-course/en/chapter5/6\n",
        "\n",
        "[2] FAISS: https://github.com/facebookresearch/faiss/wiki/Getting-started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DqrLM8iVh3y"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "czZxW3c6Vh3y"
      },
      "outputs": [],
      "source": [
        "# ! pip install datasets evaluate transformers[sentencepiece]\n",
        "\n",
        "# reference:\n",
        "# [1] https://stackoverflow.com/questions/58957169/faiss-error-could-not-find-a-version-that-satisfies-the-requirement-faiss-from/58957380\n",
        "# [1] Self-summary: \n",
        "#   1.1 Python version too high (for example: 3.13 has problem with installing faiss)\n",
        "#   1.2 must state the cuda version explicitly while installing faiss -> (after install torch) check 'nvidia-...' version in  `conda list > requirement.txt` \n",
        "# ! pip install faiss-gpu-cu12 # "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7MZuJ0ZxVh3z",
        "outputId": "924fb96f-9c2a-438b-eafd-3fec14a46cfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lephuonglantran/.conda/envs/gpuenv_2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
              "    num_rows: 3019\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kwS04TSIVh30",
        "outputId": "df1da230-3518-42c1-8936-d4d8b8d502bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter:   0%|          | 0/3019 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 3019/3019 [00:00<00:00, 8802.79 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
              "    num_rows: 808\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "issues_dataset = issues_dataset.filter(\n",
        "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
        ")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We can see that there are a lot of columns in our dataset, most of which we don’t need to build our search engine. From a search perspective, the most informative columns are `title`, `body`, and `comments`, while `html_url` provides us with a link back to the source issue. Let’s use the `Dataset.remove_columns()` function to drop the rest:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KQPnvx7pVh30",
        "outputId": "277f3ac1-4c34-46ca-ae94-c0032c045dcf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body'],\n",
              "    num_rows: 808\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "columns = issues_dataset.column_names\n",
        "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
        "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
        "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> To create our embeddings we’ll **augment each comment with the issue’s title and body**, since these fields often include useful contextual information. Because **our comments column is currently a list of comments for each issue**, we need to “`explode`” the column so that **each row consists of an (html_url, title, body, comment) tuple**. In **Pandas** we can do this with the `DataFrame.explode()` function, which creates a new row for each element in a list-like column, while replicating all the other column values. To see this in action, let’s **first switch to the Pandas DataFrame format**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Iglh2FK5Vh30"
      },
      "outputs": [],
      "source": [
        "issues_dataset.set_format(\"pandas\")\n",
        "df = issues_dataset[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_trcBwTKVh30",
        "outputId": "0d2b88e9-c8b2-42af-c7b9-0bac5df9b5a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Cool, I think we can do both :)',\n",
              " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"comments\"][0].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kVegEYscVh31"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>html_url</th>\n",
              "      <th>title</th>\n",
              "      <th>comments</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
              "      <td>Protect master branch</td>\n",
              "      <td>Cool, I think we can do both :)</td>\n",
              "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
              "      <td>Protect master branch</td>\n",
              "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
              "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
              "      <td>Backwards compatibility broken for cached data...</td>\n",
              "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
              "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
              "      <td>Backwards compatibility broken for cached data...</td>\n",
              "      <td>If it's easy enough to implement, then yes ple...</td>\n",
              "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            html_url  \\\n",
              "0  https://github.com/huggingface/datasets/issues...   \n",
              "1  https://github.com/huggingface/datasets/issues...   \n",
              "2  https://github.com/huggingface/datasets/issues...   \n",
              "3  https://github.com/huggingface/datasets/issues...   \n",
              "\n",
              "                                               title  \\\n",
              "0                              Protect master branch   \n",
              "1                              Protect master branch   \n",
              "2  Backwards compatibility broken for cached data...   \n",
              "3  Backwards compatibility broken for cached data...   \n",
              "\n",
              "                                            comments  \\\n",
              "0                    Cool, I think we can do both :)   \n",
              "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
              "2  Hi ! I guess the caching mechanism should have...   \n",
              "3  If it's easy enough to implement, then yes ple...   \n",
              "\n",
              "                                                body  \n",
              "0  After accidental merge commit (91c55355b634d0d...  \n",
              "1  After accidental merge commit (91c55355b634d0d...  \n",
              "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
              "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "comments_df = df.explode(\"comments\", ignore_index=True)\n",
        "comments_df.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Great, we can see the rows have been replicated, with the comments column containing the individual comments! **Now that we’re finished with Pandas**, we can quickly **switch back to a Dataset** by **loading the DataFrame in memory**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_R_zYbRNVh31",
        "outputId": "343f5fee-20c8-4c21-b2a3-ae9b778e5059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body'],\n",
              "    num_rows: 2964\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "comments_dataset = Dataset.from_pandas(comments_df)\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Now that we **have one comment per row**, let’s **create a new comments_length column** that **contains the number of words per comment**:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u1V7Gs2lVh31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 2964/2964 [00:00<00:00, 16523.04 examples/s]\n"
          ]
        }
      ],
      "source": [
        "comments_dataset = comments_dataset.map(\n",
        "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> We can **use this new column to filter out short comments**, which typically **include things like “cc @lewtun” or “Thanks!” that are not relevant** for our search engine. There’s **no precise number to select for the filter**, **but around 15 words** seems like a good start:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vrAOWkChVh31",
        "outputId": "d45e7de1-f384-4a1c-c997-0534f917b12e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter: 100%|██████████| 2964/2964 [00:00<00:00, 109214.77 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
              "    num_rows: 2175\n",
              "})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
        "comments_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kgHWo25UVh31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map:   0%|          | 0/2175 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 2175/2175 [00:00<00:00, 7305.89 examples/s]\n"
          ]
        }
      ],
      "source": [
        "def concatenate_text(examples):\n",
        "    return {\n",
        "        \"text\": examples[\"title\"]\n",
        "        + \" \\n \"\n",
        "        + examples[\"body\"]\n",
        "        + \" \\n \"\n",
        "        + examples[\"comments\"]\n",
        "    }\n",
        "\n",
        "\n",
        "comments_dataset = comments_dataset.map(concatenate_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
              "    num_rows: 2175\n",
              "})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "comments_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating text embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yKcqAREOVh32"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = AutoModel.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nX3lDtF-Vh32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MPNetModel(\n",
              "  (embeddings): MPNetEmbeddings(\n",
              "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): MPNetEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x MPNetLayer(\n",
              "        (attention): MPNetAttention(\n",
              "          (attn): MPNetSelfAttention(\n",
              "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (intermediate): MPNetIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): MPNetOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (relative_attention_bias): Embedding(32, 12)\n",
              "  )\n",
              "  (pooler): MPNetPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " > As we mentioned earlier, we’d **like to represent each entry in our GitHub issues corpus as a single vector**, so we **need to “pool” or average our token embeddings** in some way. One popular approach is to **perform CLS pooling on our model’s outputs**, where we **simply collect the last hidden state for the special [CLS] token**. The following function does the trick for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5fDzb5SWVh32"
      },
      "outputs": [],
      "source": [
        "def cls_pooling(model_output):\n",
        "    return model_output.last_hidden_state[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Next, we’ll create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7vnrkBahVh32"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(text_list):\n",
        "    encoded_input = tokenizer(\n",
        "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
        "    model_output = model(**encoded_input)\n",
        "    return cls_pooling(model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "27el6788Vh32",
        "outputId": "7ba1f604-edf1-434f-9ab5-5919fc13c3a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
        "embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dO9LHydvVh32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 2175/2175 [00:55<00:00, 39.39 examples/s]\n"
          ]
        }
      ],
      "source": [
        "embeddings_dataset = comments_dataset.map(\n",
        "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7RbZJfVZVh32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 323.43it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
              "    num_rows: 2175\n",
              "})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0TYuteMRVh32",
        "outputId": "06a98363-a102-4efa-9f1d-7c0666471847"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"How can I load a dataset offline?\"\n",
        "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
        "question_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "42Mxc3zUVh32"
      },
      "outputs": [],
      "source": [
        "scores, samples = embeddings_dataset.get_nearest_examples(\n",
        "    \"embeddings\", question_embedding, k=5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CEY-C86lVh32"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "samples_df = pd.DataFrame.from_dict(samples)\n",
        "samples_df[\"scores\"] = scores\n",
        "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YNm1oWXmVh32",
        "outputId": "249ba260-bbd1-4e3d-f99e-148f29e721aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
            "\n",
            "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
            "SCORE: 25.505014419555664\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
            "You can now use them offline\n",
            "```python\n",
            "datasets = load_dataset('text', data_files=data_files)\n",
            "```\n",
            "\n",
            "We'll do a new release soon\n",
            "SCORE: 24.5555419921875\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
            "\n",
            "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
            "\n",
            "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
            "\n",
            "----------\n",
            "\n",
            "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
            "\n",
            "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
            "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
            "```python\n",
            "load_dataset(\"./my_dataset\")\n",
            "```\n",
            "and the dataset script will generate your dataset once and for all.\n",
            "\n",
            "----------\n",
            "\n",
            "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
            "cf #1724 \n",
            "SCORE: 24.14897918701172\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
            "> \n",
            "> 1. (online machine)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> import datasets\n",
            "> \n",
            "> data = datasets.load_dataset(...)\n",
            "> \n",
            "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> 2. copy the dir from online to the offline machine\n",
            "> \n",
            "> 3. (offline machine)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> import datasets\n",
            "> \n",
            "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
            "> \n",
            "> ```\n",
            "> \n",
            "> \n",
            "> \n",
            "> HTH.\n",
            "\n",
            "\n",
            "SCORE: 22.893997192382812\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n",
            "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
            "1. (online machine)\n",
            "```\n",
            "import datasets\n",
            "data = datasets.load_dataset(...)\n",
            "data.save_to_disk(/YOUR/DATASET/DIR)\n",
            "```\n",
            "2. copy the dir from online to the offline machine\n",
            "3. (offline machine)\n",
            "```\n",
            "import datasets\n",
            "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
            "```\n",
            "\n",
            "HTH.\n",
            "SCORE: 22.406654357910156\n",
            "TITLE: Discussion using datasets in offline mode\n",
            "URL: https://github.com/huggingface/datasets/issues/824\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for _, row in samples_df.iterrows():\n",
        "    print(f\"COMMENT: {row.comments}\")\n",
        "    print(f\"SCORE: {row.scores}\")\n",
        "    print(f\"TITLE: {row.title}\")\n",
        "    print(f\"URL: {row.html_url}\")\n",
        "    print(\"=\" * 50)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Semantic search with FAISS (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpuenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
